---
title: "R Notebook"
output: html_notebook
---
project1 find "important happy words" in different groups

HappyDB is a corpus of 100,000 crowd-sourced happy moments via Amazon's Mechanical Turk. You can read more about it on https://arxiv.org/abs/1801.07746
In this R notebook, I would process and clean the HappyDB data and then do something interesting, just like generate word cloud to show that nowadays which thing is happiness for people, and use TFIDF (Term Frequency-Inverse Document Frequency) to select and specify their own unique words for all ages groups.

```{r}

library(ggplot2)
library(NLP)
library(tidytext)
library(tidyverse)
library(tm)
library(DT)
library(RColorBrewer)
library(wordcloud)
library(SnowballC)
library(wordcloud2)
library(jiebaRD)
library(jiebaR)

urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)

urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
combined_data <-left_join(hm_data, demo_data, by='wid')

```


first, I would like to clean the raw data. By browsing the happy data csv., I find that there are some mixed uppercase and lowercase letters, and there are also some numbers, punctuation that could not help me to analyze meaningful data text. So I want to remove them. Through using "tm" package, I could remove numbers, punctuation, uppercase and white space from the text. Then I stem the word,
```{r}
corpus = VCorpus(VectorSource(hm_data$cleaned_hm))
corpus

corpus = corpus %>%
  tm_map(content_transformer(tolower))%>%
  tm_map(removePunctuation)%>%
  tm_map(removeNumbers)%>%
  tm_map(removeWords, character(0))%>%
  tm_map(stripWhitespace)

stemmed <- tm_map(corpus, stemDocument) %>%
  tidy() %>%
  distinct()%>%
  select(text)
```

Second, I create a stop word dataset, and remove them from the text.

```{r}
data("stop_words")
word <- c("happy","ago","yesterday","lot","today","months","month",
          "happier","happiest","last","week","past","happi","time","day",
          "night", "realli","veri","abl","feel")
stop_words <- stop_words %>%
  bind_rows(mutate(tibble(word), lexicon = "updated"))

clean_words <- stemmed %>%
  mutate(id = row_number()) %>%
  distinct()%>%
  unnest_tokens(stems, text) %>%
  anti_join(stop_words, by = c("stems" = "word"))
```

Then I would like to use the clean_word list to generate the total word cloud. First I count the frequency of all clean words, then I use wordcloud2() to generate the word cloud picture.

```{r}
completed_freq = freq(clean_words$stems)
sorted_index = order(completed_freq$freq, decreasing = T)
completed_freq = completed_freq[sorted_index, ]

wordcloud2(completed_freq, minSize = 5)
```

After generate the total word cloud, I suppose that this word cloud could not represent the happy moment of all people, such as different ages or different countries. Hence I want to expose the specific happy moment for those specific people.
However, it is easy to slice the total data into different groups and to independently generate the word cloud. But, we could see, some wide words, such as "friend" "home" "family", they could not represent specific groups of people, They're all too common.
Above all, I find that I could use TFIDF method to find some words that could represent specific groups of people. ( I will use different age groups as an example.)

First, I divide all ages into three groups, young(age <= 30), middle(30 <age <= 50), and old(age > 50). And then I clean these data as above.
```{r}
young_data <- combined_data[combined_data$age <= 30, ]
middle_data <- combined_data[30 < combined_data$age & combined_data$age <= 50, ]
old_data <- combined_data[combined_data$age > 50, ]

young_data = select(young_data,cleaned_hm)
middle_data = select(middle_data,cleaned_hm)
old_data = select(old_data,cleaned_hm)

corpus_y = VCorpus(VectorSource(young_data$cleaned_hm))

corpus_y = corpus_y %>%
  tm_map(content_transformer(tolower))%>%
  tm_map(removePunctuation)%>%
  tm_map(removeNumbers)%>%
  tm_map(removeWords, character(0))%>%
  tm_map(stripWhitespace)
stemmed_y <- tm_map(corpus_y, stemDocument) %>%
  tidy() %>%
  distinct()%>%
  select(text)
completed_y <- stemmed_y %>%
  mutate(id = row_number()) %>%
  distinct()%>%
  unnest_tokens(stems, text) %>%
  anti_join(stop_words, by = c("stems" = "word"))
completed_y_freq = freq(completed_y$stems)
sorted_y_index = order(completed_y_freq$freq, decreasing = T)
completed_y_freq = completed_y_freq[sorted_y_index, ]

attach(completed_y_freq)
completed_y_freq$id <- 1
colnames(completed_y_freq) <- c("words", "n", "id")
rownames(completed_y_freq) <- NULL

corpus_m = VCorpus(VectorSource(middle_data$cleaned_hm))
corpus_m = corpus_m %>%
  tm_map(content_transformer(tolower))%>%
  tm_map(removePunctuation)%>%
  tm_map(removeNumbers)%>%
  tm_map(removeWords, character(0))%>%
  tm_map(stripWhitespace)
stemmed_m <- tm_map(corpus_m, stemDocument) %>%
  tidy() %>%
  distinct()%>%
  select(text)
completed_m <- stemmed_m %>%
  mutate(id = row_number()) %>%
  distinct()%>%
  unnest_tokens(stems, text) %>%
  anti_join(stop_words, by = c("stems" = "word"))
completed_m_freq = freq(completed_m$stems)
sorted_m_index = order(completed_m_freq$freq, decreasing = T)
completed_m_freq = completed_m_freq[sorted_m_index, ]

attach(completed_m_freq)
completed_m_freq$id <- 2
colnames(completed_m_freq) <- c("words", "n", "id")
rownames(completed_m_freq) <- NULL

combined_by_row <- rbind(completed_y_freq,completed_m_freq )

corpus_o = VCorpus(VectorSource(old_data$cleaned_hm))
corpus_o = corpus_o %>%
  tm_map(content_transformer(tolower))%>%
  tm_map(removePunctuation)%>%
  tm_map(removeNumbers)%>%
  tm_map(removeWords, character(0))%>%
  tm_map(stripWhitespace)
stemmed_o <- tm_map(corpus_o, stemDocument) %>%
  tidy() %>%
  distinct()%>%
  select(text)
completed_o <- stemmed_o %>%
  mutate(id = row_number()) %>%
  distinct()%>%
  unnest_tokens(stems, text) %>%
  anti_join(stop_words, by = c("stems" = "word"))
completed_o_freq = freq(completed_o$stems)
sorted_o_index = order(completed_o_freq$freq, decreasing = T)
completed_o_freq = completed_o_freq[sorted_o_index, ]

attach(completed_o_freq)
completed_o_freq$id <- 3
colnames(completed_o_freq) <- c("words", "n", "id")
rownames(completed_o_freq) <- NULL

combined_by_row <- rbind(combined_by_row,completed_o_freq )

```

Then I use 'tidytext' package to compute the value of TFIDF in three ages groups. As a result, we could see some different important happy words in young group, middle group and old group.
```{r}
combined_by_row %>%
  bind_tf_idf(term = words,document = id,n = n) -> tf_idf_table

tf_idf_table %>% 
  group_by(id) %>% 
  top_n(10,tf_idf) %>% 
  ungroup() -> top10
top10
```

From this data table, we could see that young people usually use 'bliss' 'virtue' and 'stimuli' to be their happy words, (a little more simple words) and middle aged people often use 'judgement' 'identify' 'flourish',(such words seems that they would look at things more rationally, huh), and in the end, the old people often use 'grandchildren' 'boiler' 'air conditioner'.

Then I would do some data visualization based on the calculated TFIDF values.
```{r}
top10 %>% 
  count(words) %>%
  wordcloud2(color = ifelse(top10[, 3] == 1, 'red', 'skyblue'), size = 0.5,   shape = 'diamond')
```

```{r}
top10 %>% 
  count(words) %>%
  wordcloud2(color = ifelse(top10[, 3] == 2, 'orange', 'skyblue'), size = 0.5,  shape = 'diamond')
```

```{r}
top10 %>% 
  count(words) %>%
  wordcloud2(color = ifelse(top10[, 3] == 3, 'purple', 'skyblue'), size = 0.5, shape = 'diamond')
```

```{r}
ggplot(top10[1:15,],aes(x = words, y = tf_idf)) +
  geom_bar(stat = "identity", fill = "green")

ggplot(top10[16:25,],aes(x = words, y = tf_idf)) +
  geom_bar(stat = "identity", fill = "blue")

ggplot(top10[26:40,],aes(x = words, y = tf_idf)) +
  geom_bar(stat = "identity", fill = "yellow")

```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.



